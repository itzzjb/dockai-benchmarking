name: Compare DockAI vs Human vs CNB (Academic Benchmark)

on:
  workflow_dispatch:
    inputs:
      skip_dockai:
        description: "Skip DockAI generation (for testing)"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"
  push:
    branches: [main]
    paths:
      - config.json

jobs:
  compare:
    name: Empirical Evaluation (Size, Time, Security)
    runs-on: ubuntu-latest
    permissions:
      contents: read
    env:
      TARGET_DIR: target-repo
      REPORT_PATH: report.md
      HUMAN_IMG: human:compare
      DOCKAI_IMG: dockai:compare
      CNB_IMG: cnb:compare
      CODEX_IMG: codex:compare
      PACK_VERSION: 0.36.2

    steps:
      - name: Checkout workflow repo
        uses: actions/checkout@v6.0.1

      # Cache apt packages to avoid repeated downloads
      - name: Cache apt packages
        uses: actions/cache@v4
        with:
          path: /var/cache/apt/archives
          key: ${{ runner.os }}-apt-${{ hashFiles('.github/workflows/compare-builds.yml') }}
          restore-keys: |
            ${{ runner.os }}-apt-

      - name: Install dependencies
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y jq bc

      # Cache pip packages for faster Google Sheets integration
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: ".github/requirements.txt"

      - name: Set up Trivy CLI (0.68.1)
        uses: aquasecurity/setup-trivy@e6c2c5e321ed9123bda567646e2f96565e34abe1
        with:
          version: v0.68.1
          cache: true

      - name: Set up pack CLI
        uses: buildpacks/github-actions/setup-pack@v5.9.7
        with:
          pack-version: ${{ env.PACK_VERSION }}

      - name: Resolve repository and branch
        id: repo
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          input_repo="${{ github.event.inputs.repo_url }}"
          input_branch="${{ github.event.inputs.branch }}"

          # Fallback to config.json if input is empty
          # (Ensure config.json exists or ignore errors if it doesn't)
          if [ -f config.json ]; then
            config_repo=$(jq -r '.repository_url // empty' config.json)
          else
            config_repo=""
          fi

          raw_repo="${input_repo:-$config_repo}"

          if [ -z "$raw_repo" ]; then echo "Repo URL required" >&2; exit 1; fi

          # --- FIX: SMART URL DETECTION ---
          if [[ "$raw_repo" =~ ^https?:// ]]; then
            clone_url="$raw_repo"
          else
            clone_url="https://github.com/${raw_repo}"
          fi

          # --- FIX: ROBUST AUTH INSERTION ---
          auth_clone_url="$clone_url"
          if [ -n "${GITHUB_TOKEN:-}" ]; then
             # Strip existing protocol (http:// or https://) to insert credentials
             clean_url="${clone_url#*://}"
             auth_clone_url="https://${GITHUB_ACTOR:-github-actions}:${GITHUB_TOKEN}@${clean_url}"
          fi

          echo "clone_url=$clone_url" >> "$GITHUB_OUTPUT"
          echo "auth_clone_url=$auth_clone_url" >> "$GITHUB_OUTPUT"
          echo "repo_label=$raw_repo" >> "$GITHUB_OUTPUT"
          # Derive repository name without protocol/host or .git suffix, then fold owner/repo into a single safe name
          repo_path=$(echo "$raw_repo" \
            | sed 's#^[^:/]*://##' \
            | sed 's#^[^@]*@##' \
            | sed 's#^[^:]*:##' \
            | sed 's#/$##' \
            | sed 's#\.git$##')

          # Replace slashes with hyphens to include owner and sanitize spaces
          repo_name=$(echo "$repo_path" | tr '/' '-' | tr ' ' '-')
          if [ -z "$repo_name" ]; then repo_name="${TARGET_DIR}"; fi
          echo "repo_name=$repo_name" >> "$GITHUB_OUTPUT"
          echo "branch=${input_branch:-main}" >> "$GITHUB_OUTPUT"

      - name: Checkout target repo
        run: |
          set -euo pipefail
          rm -rf "${TARGET_DIR}"
          git clone --depth 1 "${{ steps.repo.outputs.auth_clone_url }}" "${TARGET_DIR}"
          # Reset remote to avoid token leak
          git -C "${TARGET_DIR}" remote set-url origin "${{ steps.repo.outputs.clone_url }}"

      - name: Backup human Dockerfile
        run: |
          if [ -f "${TARGET_DIR}/Dockerfile" ]; then
            mv "${TARGET_DIR}/Dockerfile" "${TARGET_DIR}/../Dockerfile.human.bak"
          fi
          rm -f "${TARGET_DIR}"/Dockerfile.*

      - name: Generate Codex CLI Dockerfile
        continue-on-error: true
        run: |
          set -euo pipefail
          npm install -g @codexai/cli || true

          cd "${TARGET_DIR}"
          if command -v codex &> /dev/null; then
            codex dockerfile generate --output Dockerfile.codex || echo "Codex generation failed"
          else
            echo "Codex CLI not available, skipping generation"
          fi
          cd ..

      - name: Set up Docker Buildx (for DockAI)
        uses: docker/setup-buildx-action@v3.11.1
        with:
          driver-opts: |
            image=moby/buildkit:latest
            network=host

      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      - name: Verify Docker daemon and warm images
        run: |
          set -euo pipefail
          sudo systemctl start docker || true
          docker info
          docker version

      # Cache UV dependencies (CUDA/PyTorch packages for DockAI) to avoid 15-minute downloads
      - name: Cache UV packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            /home/runner/work/_temp/setup-uv-cache
          key: ${{ runner.os }}-uv-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Restore original Dockerfile
        run: |
          if [ -f "${TARGET_DIR}/../Dockerfile.human.bak" ]; then
            cp "${TARGET_DIR}/../Dockerfile.human.bak" "${TARGET_DIR}/Dockerfile"
          fi

      - name: Generate DockAI Dockerfile
        if: github.event.inputs.skip_dockai != 'true'
        uses: itzzjb/dockai@v4
        continue-on-error: true
        with:
          openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          project_path: ${{ env.TARGET_DIR }}
          max_retries: 10
          langchain_tracing_v2: true
          langchain_api_key: ${{ secrets.LANGCHAIN_API_KEY }}
          langchain_project: dockai
          llm_provider: openai
          model_analyzer: gpt-5-mini
          model_blueprint: gpt-5-mini
          model_generator: gpt-5-mini
          model_generator_iterative: gpt-5-mini
          model_reviewer: gpt-5-mini
          model_reflector: gpt-5-mini
          model_error_analyzer: gpt-5-mini
          model_iterative_improver: gpt-5-mini

      - name: Organize Dockerfiles
        run: |
          if [ -f "${TARGET_DIR}/Dockerfile" ]; then
            cp "${TARGET_DIR}/Dockerfile" "${TARGET_DIR}/Dockerfile.dockai"
          fi
          if [ -f "${TARGET_DIR}/../Dockerfile.human.bak" ]; then
            cp "${TARGET_DIR}/../Dockerfile.human.bak" "${TARGET_DIR}/Dockerfile"
          fi
          rm -f "${TARGET_DIR}/../Dockerfile.human.bak"

      - name: Purge study images (keep action images)
        if: always()
        run: |
          set -euo pipefail
          # Remove only the study images to avoid deleting GitHub Action images
          docker image rm -f "${HUMAN_IMG}" "${DOCKAI_IMG}" "${CNB_IMG}" "${CODEX_IMG}" 2>/dev/null || true
          # Optional: clean BuildKit cache without touching pulled action images
          docker builder prune -af || true

      - name: Check Artifact Existence
        id: detect
        run: |
          [ -f "${TARGET_DIR}/Dockerfile" ] && echo "human_present=true" >> "$GITHUB_OUTPUT" || echo "human_present=false" >> "$GITHUB_OUTPUT"
          [ -f "${TARGET_DIR}/Dockerfile.dockai" ] && echo "dockai_present=true" >> "$GITHUB_OUTPUT" || echo "dockai_present=false" >> "$GITHUB_OUTPUT"
          [ -f "${TARGET_DIR}/Dockerfile.codex" ] && echo "codex_present=true" >> "$GITHUB_OUTPUT" || echo "codex_present=false" >> "$GITHUB_OUTPUT"

      - name: Hadolint Human Dockerfile
        if: steps.detect.outputs.human_present == 'true'
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: ${{ env.TARGET_DIR }}/Dockerfile
          format: json
          output-file: hadolint-human.json
          failure-threshold: error
          no-fail: true

      - name: Hadolint DockAI Dockerfile
        if: steps.detect.outputs.dockai_present == 'true'
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: ${{ env.TARGET_DIR }}/Dockerfile.dockai
          format: json
          output-file: hadolint-dockai.json
          failure-threshold: error
          no-fail: true

      - name: Hadolint Codex Dockerfile
        if: steps.detect.outputs.codex_present == 'true'
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: ${{ env.TARGET_DIR }}/Dockerfile.codex
          format: json
          output-file: hadolint-codex.json
          failure-threshold: error
          no-fail: true

      - name: Stub Hadolint for CNB (no Dockerfile)
        run: echo "[]" > hadolint-cnb.json

      # --- BUILD & TIME MEASUREMENT STEPS ---

      - name: Build Human (Measure Time)
        if: steps.detect.outputs.human_present == 'true'
        run: |
          # Continue even if the build fails so later steps can still run
          set -u -o pipefail
          start=$(date +%s)
          if docker build --no-cache --file "${TARGET_DIR}/Dockerfile" --tag "${HUMAN_IMG}" "${TARGET_DIR}"; then
            status="success"
          else
            status="failed"
          fi
          end=$(date +%s)
          echo "$((end-start))" > human_time.txt
          echo "$status" > human_status.txt

      - name: Build DockAI (Measure Time)
        if: steps.detect.outputs.dockai_present == 'true'
        run: |
          set -u -o pipefail
          start=$(date +%s)
          if docker build --no-cache --file "${TARGET_DIR}/Dockerfile.dockai" --tag "${DOCKAI_IMG}" "${TARGET_DIR}"; then
            status="success"
          else
            status="failed"
          fi
          end=$(date +%s)
          echo "$((end-start))" > dockai_time.txt
          echo "$status" > dockai_status.txt

      - name: Build Codex (Measure Time)
        if: steps.detect.outputs.codex_present == 'true'
        run: |
          set -u -o pipefail
          start=$(date +%s)
          if docker build --no-cache --file "${TARGET_DIR}/Dockerfile.codex" --tag "${CODEX_IMG}" "${TARGET_DIR}"; then
            status="success"
          else
            status="failed"
          fi
          end=$(date +%s)
          echo "$((end-start))" > codex_time.txt
          echo "$status" > codex_status.txt

      - name: Build CNB (Measure Time)
        run: |
          set -u -o pipefail
          start=$(date +%s)
          if pack build "${CNB_IMG}" --builder paketobuildpacks/builder:base --path "${TARGET_DIR}" --pull-policy if-not-present --clear-cache; then
            status="success"
          else
            status="failed"
          fi
          end=$(date +%s)
          echo "$((end-start))" > cnb_time.txt
          echo "$status" > cnb_status.txt

      # --- METRIC COLLECTION ---

      - name: Trivy Scans
        if: always()
        run: |
          # Function to run trivy safely
          run_trivy() {
            local img=$1
            local out=$2
            if docker image inspect "$img" >/dev/null 2>&1; then
              trivy image --format json --output "$out" --severity CRITICAL,HIGH,MEDIUM,LOW "$img"
            else
              echo "{}" > "$out"
            fi
          }
          run_trivy "${HUMAN_IMG}" trivy-human.json
          run_trivy "${DOCKAI_IMG}" trivy-dockai.json
          run_trivy "${CODEX_IMG}" trivy-codex.json
          run_trivy "${CNB_IMG}" trivy-cnb.json

      - name: Calculate Academic Metrics (C_total)
        if: always()
        id: math
        env:
          W_SIZE: 0.3
          W_TIME: 0.2
          W_SEC: 0.5
        run: |
          set -e

          # 1. ROBUST METRIC EXTRACTION
          get_status() { [ -f "$1" ] && cat "$1" || echo "missing"; }
          get_size() { docker image inspect "$1" --format '{{.Size}}' 2>/dev/null || echo 0; }
          get_time() { cat "$1" 2>/dev/null || echo 0; }

          # Security Index Calculation (Fixed for null .Results)
          get_omega() {
            local f=$1
            # Check if file exists and has content
            if [ ! -s "$f" ]; then echo 0; return; fi
            
            # Run jq with error handling (|| echo 0)
            # Added '?' after .Results[] to prevent crash if Results is null
            jq -r '
              reduce (.Results[]?.Vulnerabilities[]? // empty) as $v (0; 
                if $v.Severity == "CRITICAL" then . + 10
                elif $v.Severity == "HIGH" then . + 5
                elif $v.Severity == "MEDIUM" then . + 2
                elif $v.Severity == "LOW" then . + 1
                else . end
              )
            ' "$f" || echo 0
          }

          # Hadolint error/warning counters (safe on missing/empty files)
          get_lint_counts() {
            local f="$1"
            if [ ! -s "$f" ]; then echo "0 0"; return; fi
            jq -r '
              reduce .[]? as $i ([0,0];
                if ($i.level == "error") then [.[0]+1, .[1]]
                elif ($i.level == "warning" or $i.level == "info") then [.[0], .[1]+1]
                else . end
              ) | @tsv
            ' "$f" 2>/dev/null || echo "0 0"
          }

          calc_penalty() {
            local errs="${1:-0}"
            local warns="${2:-0}"
            awk -v e="$errs" -v w="$warns" 'BEGIN {printf "%.4f", (e*0.1) + (w*0.05)}'
          }

          calc_final() {
            local base="${1:-0}"
            local pen="${2:-0}"
            awk -v b="$base" -v p="$pen" 'BEGIN {printf "%.4f", b * (1 + p)}'
          }

          # Get Raw Values
          h_status=$(get_status human_status.txt); d_status=$(get_status dockai_status.txt); x_status=$(get_status codex_status.txt); c_status=$(get_status cnb_status.txt)
          h_size=$(get_size "${HUMAN_IMG}"); h_time=$(get_time human_time.txt); h_omega=$(get_omega trivy-human.json)
          d_size=$(get_size "${DOCKAI_IMG}"); d_time=$(get_time dockai_time.txt); d_omega=$(get_omega trivy-dockai.json)
          x_size=$(get_size "${CODEX_IMG}"); x_time=$(get_time codex_time.txt); x_omega=$(get_omega trivy-codex.json)
          c_size=$(get_size "${CNB_IMG}");    c_time=$(get_time cnb_time.txt);   c_omega=$(get_omega trivy-cnb.json)

          # Lint counts (tolerate empty output from jq)
          if ! read h_err h_warn <<< "$(get_lint_counts hadolint-human.json)"; then h_err=0; h_warn=0; fi
          if ! read d_err d_warn <<< "$(get_lint_counts hadolint-dockai.json)"; then d_err=0; d_warn=0; fi
          if ! read x_err x_warn <<< "$(get_lint_counts hadolint-codex.json)"; then x_err=0; x_warn=0; fi
          if ! read c_err c_warn <<< "$(get_lint_counts hadolint-cnb.json)"; then c_err=0; c_warn=0; fi

          # Coerce missing or non-positive values to safe defaults to avoid awk/bc errors
          default_zero() { local v="$1"; [ -z "$v" ] && echo 0 || echo "$v"; }
          default_one() {
            local v="$1"
            if [ -z "$v" ]; then echo 1; elif [ "$v" -le 0 ] 2>/dev/null; then echo 1; else echo "$v"; fi
          }

          h_size=$(default_zero "$h_size"); h_time=$(default_zero "$h_time"); h_omega=$(default_zero "$h_omega")
          d_size=$(default_zero "$d_size"); d_time=$(default_zero "$d_time"); d_omega=$(default_zero "$d_omega")
          x_size=$(default_zero "$x_size"); x_time=$(default_zero "$x_time"); x_omega=$(default_zero "$x_omega")
          c_size=$(default_one "$c_size");  c_time=$(default_one "$c_time");  c_omega=$(default_one "$c_omega")

          h_err=$(default_zero "$h_err"); h_warn=$(default_zero "$h_warn")
          d_err=$(default_zero "$d_err"); d_warn=$(default_zero "$d_warn")
          x_err=$(default_zero "$x_err"); x_warn=$(default_zero "$x_warn")
          c_err=$(default_zero "$c_err"); c_warn=$(default_zero "$c_warn")

          # 2. ZERO-DIVISION PROTECTION (For Baseline)
          # If any baseline metric is 0, set it to 1 to allow division
          [ "$c_size" -le 0 ] && c_size=1
          [ "$c_time" -le 0 ] && c_time=1
          [ "$c_omega" -le 0 ] && c_omega=1

          # 2b. Normalization baseline: if CNB failed, fall back to best successful build
          norm_size=$c_size; norm_time=$c_time; norm_omega=$c_omega
          if [ "$c_status" != "success" ]; then
            norm_size=0; norm_time=0; norm_omega=0
            if [ "$h_status" = "success" ]; then
              norm_size=$h_size; norm_time=$h_time; norm_omega=$h_omega
            fi
            if [ "$d_status" = "success" ]; then
              { [ "$norm_size" -le 0 ] || [ "$d_size" -lt "$norm_size" ]; } && norm_size=$d_size
              { [ "$norm_time" -le 0 ] || [ "$d_time" -lt "$norm_time" ]; } && norm_time=$d_time
              { [ "$norm_omega" -le 0 ] || [ "$d_omega" -lt "$norm_omega" ]; } && norm_omega=$d_omega
            fi
            if [ "$x_status" = "success" ]; then
              { [ "$norm_size" -le 0 ] || [ "$x_size" -lt "$norm_size" ]; } && norm_size=$x_size
              { [ "$norm_time" -le 0 ] || [ "$x_time" -lt "$norm_time" ]; } && norm_time=$x_time
              { [ "$norm_omega" -le 0 ] || [ "$x_omega" -lt "$norm_omega" ]; } && norm_omega=$x_omega
            fi
            [ "$norm_size" -le 0 ] && norm_size=1
            [ "$norm_time" -le 0 ] && norm_time=1
            [ "$norm_omega" -le 0 ] && norm_omega=1
          fi

          echo "DEBUG: Baseline - Size:$c_size Time:$c_time Omega:$c_omega"

          # 3. CALCULATE METRICS (Awk)
          calc_metrics() {
             # Ensure inputs are not empty strings, default to 0
             local s="${1:-0}"
             local t="${2:-0}"
             local w="${3:-0}"
             
             awk -v s="$s" -v t="$t" -v w="$w" \
                 -v bs="$4" -v bt="$5" -v bw="$6" \
                 -v ws="$W_SIZE" -v wt="$W_TIME" -v ww="$W_SEC" '
             BEGIN {
               # Normalize (Value / Baseline)
               ns = s / bs
               nt = t / bt
               nw = w / bw
               
               # Composite Cost Function
               ctotal = (ws * ns) + (wt * nt) + (ww * nw)
               
               printf "%.4f %.4f %.4f %.4f", ctotal, ns, nt, nw
             }'
          }

          # Calculate Human, DockAI & Codex Scores (explicitly capture output, tolerate failures)
          if output=$(calc_metrics "$h_size" "$h_time" "$h_omega" "$norm_size" "$norm_time" "$norm_omega" 2>/dev/null); then
            set -- $output
            h_score=${1:-0}; h_ns=${2:-0}; h_nt=${3:-0}; h_nw=${4:-0}
          else
            h_score=0; h_ns=0; h_nt=0; h_nw=0
          fi

          if output=$(calc_metrics "$d_size" "$d_time" "$d_omega" "$norm_size" "$norm_time" "$norm_omega" 2>/dev/null); then
            set -- $output
            d_score=${1:-0}; d_ns=${2:-0}; d_nt=${3:-0}; d_nw=${4:-0}
          else
            d_score=0; d_ns=0; d_nt=0; d_nw=0
          fi

          if output=$(calc_metrics "$x_size" "$x_time" "$x_omega" "$norm_size" "$norm_time" "$norm_omega" 2>/dev/null); then
            set -- $output
            x_score=${1:-0}; x_ns=${2:-0}; x_nt=${3:-0}; x_nw=${4:-0}
          else
            x_score=0; x_ns=0; x_nt=0; x_nw=0
          fi

          c_score="1.0000"

          # 4. HADOLINT PENALTIES AND FINAL COST (Multiplicative: C_final = C_total Ã— (1 + P_quality))
          h_penalty=$(calc_penalty "$h_err" "$h_warn")
          d_penalty=$(calc_penalty "$d_err" "$d_warn")
          x_penalty=$(calc_penalty "$x_err" "$x_warn")
          c_penalty=$(calc_penalty "$c_err" "$c_warn")

          h_final=$(calc_final "$h_score" "$h_penalty")
          d_final=$(calc_final "$d_score" "$d_penalty")
          x_final=$(calc_final "$x_score" "$x_penalty")
          c_final=$(calc_final "$c_score" "$c_penalty")

          # Disqualify builds that did not finish
          dq_score="9999"
          if [ "$h_status" != "success" ]; then h_final=$dq_score; h_score=$dq_score; fi
          if [ "$d_status" != "success" ]; then d_final=$dq_score; d_score=$dq_score; fi
          if [ "$x_status" != "success" ]; then x_final=$dq_score; x_score=$dq_score; fi
          if [ "$c_status" != "success" ]; then c_final=$dq_score; c_score=$dq_score; fi

          # Safe float comparison that tolerates bc parse failures
          safe_lt() {
            local a="${1:-}"
            local b="${2:-}"
            local cmp
            cmp=$(echo "$a < $b" | bc -l 2>/dev/null || echo 0)
            [ "$cmp" -gt 0 ]
          }

          # 5. DETERMINE WINNER (uses final score including penalty)
          winner="No successful builds"
          lowest=""

          if [ "$c_status" = "success" ]; then
            winner="CNB (Baseline)"; lowest=$c_final
          fi

          if [ "$h_status" = "success" ] && { [ -z "$lowest" ] || safe_lt "$h_final" "$lowest"; }; then
            lowest=$h_final; winner="Human"
          fi

          if [ "$d_status" = "success" ] && { [ -z "$lowest" ] || safe_lt "$d_final" "$lowest"; }; then
            lowest=$d_final; winner="DockAI"
          fi

          if [ "$x_status" = "success" ] && { [ -z "$lowest" ] || safe_lt "$x_final" "$lowest"; }; then
            lowest=$x_final; winner="Codex"
          fi

          # Export
          echo "winner=$winner" >> "$GITHUB_ENV"

          # Debug output
          echo "Scores (base) -> CNB: $c_score, Human: $h_score, DockAI: $d_score, Codex: $x_score"
          echo "Penalties -> CNB: $c_penalty, Human: $h_penalty, DockAI: $d_penalty, Codex: $x_penalty"
          echo "Final -> CNB: $c_final, Human: $h_final, DockAI: $d_final, Codex: $x_final"

          # Generate JSON
          cat <<EOF > results.json
          {
            "baseline": {"status": "$c_status", "size": $c_size, "time": $c_time, "omega": $c_omega, "errors": $c_err, "warnings": $c_warn, "penalty": $c_penalty, "cost": $c_final},
            "human": {"status": "$h_status", "size": $h_size, "time": $h_time, "omega": $h_omega, "errors": $h_err, "warnings": $h_warn, "penalty": $h_penalty, "cost": $h_final},
            "dockai": {"status": "$d_status", "size": $d_size, "time": $d_time, "omega": $d_omega, "errors": $d_err, "warnings": $d_warn, "penalty": $d_penalty, "cost": $d_final},
            "codex": {"status": "$x_status", "size": $x_size, "time": $x_time, "omega": $x_omega, "errors": $x_err, "warnings": $x_warn, "penalty": $x_penalty, "cost": $x_final},
            "notes": {"disqualified_score": $dq_score, "winner": "$winner"}
          }
          EOF

      - name: Publish GitHub Summary
        if: always()
        run: |
          winner=${winner:-$(jq -r '.notes.winner // "Unknown"' results.json)}
          cat >> "$GITHUB_STEP_SUMMARY" <<EOF
          ## Build-off Summary
          | Method | Status | Cost (C_final) |
          | :--- | :--- | :--- |
          | CNB (Baseline) | $(jq -r '.baseline.status' results.json) | $(jq -r '.baseline.cost' results.json) |
          | Human | $(jq -r '.human.status' results.json) | $(jq -r '.human.cost' results.json) |
          | DockAI | $(jq -r '.dockai.status' results.json) | $(jq -r '.dockai.cost' results.json) |
          | Codex | $(jq -r '.codex.status' results.json) | $(jq -r '.codex.cost' results.json) |

          **Winner:** ${winner}
          EOF

      - name: Generate Academic Report
        if: always()
        run: |
          # Convert sizes to MB for display
          to_mb() { awk -v v="$1" 'BEGIN {printf "%.2f", v/1024/1024}'; }

          # Read JSON
          hs=$(jq '.human.size' results.json); ht=$(jq '.human.time' results.json); ho=$(jq '.human.omega' results.json); hc=$(jq '.human.cost' results.json); hs_status=$(jq -r '.human.status' results.json)
          ds=$(jq '.dockai.size' results.json); dt=$(jq '.dockai.time' results.json); do=$(jq '.dockai.omega' results.json); dc=$(jq '.dockai.cost' results.json); ds_status=$(jq -r '.dockai.status' results.json)
          xs=$(jq '.codex.size' results.json); xt=$(jq '.codex.time' results.json); xo=$(jq '.codex.omega' results.json); xc=$(jq '.codex.cost' results.json); xs_status=$(jq -r '.codex.status' results.json)
          cs=$(jq '.baseline.size' results.json); ct=$(jq '.baseline.time' results.json); co=$(jq '.baseline.omega' results.json); bc=$(jq '.baseline.cost' results.json); cs_status=$(jq -r '.baseline.status' results.json)

          cat > "${REPORT_PATH}" <<EOF
          # Empirical Evaluation Report
          **Method:** Composite Optimization Metric (\$C_{total}\$) with Hadolint Penalty (\$C_{final}\$)
          **Winner:** ${winner}

          ## 1. Executive Summary (Lower \$C_{final}\$ is Better)
          | Method | Size (MB) | Build Time (s) | Sec. Index (\$\\Omega\$) | **\$C_{final}\$ Score** |
          | :--- | :--- | :--- | :--- | :--- |
          | **CNB (Baseline)** | $(to_mb $cs) | $ct | $co | **$bc** (Status: $cs_status) |
          | **Human** | $(to_mb $hs) | $ht | $ho | **$hc** (Status: $hs_status) |
          | **DockAI (Ours)** | $(to_mb $ds) | $dt | $do | **$dc** (Status: $ds_status) |
          | **Codex** | $(to_mb $xs) | $xt | $xo | **$xc** (Status: $xs_status) |

          ## 2. Mathematical Definition
          The **Composite Optimization Metric (\$C_{total}\$)** is calculated as:
          \$\$C_{total} = 0.3 \\cdot \\hat{S} + 0.2 \\cdot \\hat{T} + 0.5 \\cdot \\hat{V}\$\$

          The **Static Analysis Penalty** from Hadolint is:
          \$\$P_{quality} = 0.1 \\cdot Errors + 0.05 \\cdot Warnings\$\$

          The final score uses multiplicative scaling to keep penalties proportional:
          \$\$C_{final} = C_{total} \\times (1 + P_{quality})\$\$

          Where:
          * \$\hat{S}, \hat{T}, \hat{V}\$ are normalized ratios against the CNB baseline.
          * Security Index (\$\Omega\$) = \$(10 \times Crit) + (5 \times High) + (2 \times Med) + (1 \times Low)\$
          * Hadolint Errors/Warnings are counted from the JSON lint reports.

          ## 3. Raw Data (For Paper)
          \`\`\`json
          $(cat results.json)
          \`\`\`
          EOF

      - name: Assemble Artifact Layout
        if: always()
        env:
          REPO_NAME: ${{ steps.repo.outputs.repo_name }}
        run: |
          set -euo pipefail
          ARTIFACT_ROOT="artifact/${REPO_NAME}"
          DOCKERFILES_DIR="$ARTIFACT_ROOT/dockerfiles"
          DATA_DIR="$ARTIFACT_ROOT/data"

          rm -rf "$ARTIFACT_ROOT"
          mkdir -p "$DOCKERFILES_DIR" "$DATA_DIR"

          # Keep the report at the artifact root
          if [ -f "${REPORT_PATH}" ]; then
            cp "${REPORT_PATH}" "$ARTIFACT_ROOT/"
          fi

          # Collect Dockerfiles
          if [ -f "${TARGET_DIR}/Dockerfile" ]; then
            cp "${TARGET_DIR}/Dockerfile" "$DOCKERFILES_DIR/Dockerfile.human"
          fi
          if [ -f "${TARGET_DIR}/Dockerfile.dockai" ]; then
            cp "${TARGET_DIR}/Dockerfile.dockai" "$DOCKERFILES_DIR/Dockerfile.dockai"
          fi
          if [ -f "${TARGET_DIR}/Dockerfile.codex" ]; then
            cp "${TARGET_DIR}/Dockerfile.codex" "$DOCKERFILES_DIR/Dockerfile.codex"
          fi

          # Gather remaining outputs under data/
          for f in results.json trivy-*.json hadolint-*.json *_time.txt *_status.txt; do
            for g in $f; do
              [ -e "$g" ] || continue
              cp "$g" "$DATA_DIR/"
            done
          done

      - name: Upload Research Data
        if: always()
        uses: actions/upload-artifact@v5.0.0
        with:
          name: research-data-${{ steps.repo.outputs.repo_name }}
          path: artifact/${{ steps.repo.outputs.repo_name }}

      - name: Publish Metrics to Google Sheet
        if: always()
        env:
          SHEET_ID: ${{ secrets.GDRIVE_SHEET_ID }}
          SA_KEY: ${{ secrets.GDRIVE_SERVICE_ACCOUNT_KEY }}
          REPO_NAME: ${{ steps.repo.outputs.repo_name }}
          BRANCH_NAME: ${{ steps.repo.outputs.branch }}
        run: |
          set -euo pipefail

          if [ -z "${SHEET_ID:-}" ]; then
            echo "SHEET_ID not set; skipping sheet update" >&2
            exit 0
          fi

          echo "$SA_KEY" > /tmp/sa.json
          python -m pip install --quiet -r .github/requirements.txt

          cat <<'PY' > /tmp/update_sheet.py
          import datetime
          import json
          import os
          import sys

          import gspread
          from gspread.utils import rowcol_to_a1
          from google.oauth2.service_account import Credentials

          sheet_id = os.environ.get("SHEET_ID")
          sheet_tab = os.environ.get("SHEET_TAB")
          repo_name = os.environ.get("REPO_NAME", "unknown")
          branch_name = os.environ.get("BRANCH_NAME", "unknown")

          try:
              with open("results.json", "r", encoding="utf-8") as f:
                  data = json.load(f)
          except FileNotFoundError:
              print("results.json not found; skipping sheet update", file=sys.stderr)
              sys.exit(0)

          def get(obj, *keys, default=""):
              cur = obj
              for k in keys:
                  if not isinstance(cur, dict) or k not in cur:
                      return default
                  cur = cur[k]
              return cur

          def to_mb(val):
              try:
                  return round(float(val) / 1024 / 1024, 2)
              except Exception:
                  return ""

          def format_sheet(ws, headers):
              # Apply simple readability formatting; best-effort so errors are non-fatal
              try:
                  last_col_a1 = rowcol_to_a1(1, len(headers))
                  last_col_letter = last_col_a1.rstrip("1")
                  header_range = f"A1:{last_col_letter}1"
                  data_range = f"A2:{last_col_letter}{ws.row_count}"

                  ws.freeze(rows=1)
                  ws.set_basic_filter(f"A1:{last_col_letter}{ws.row_count}")

                  ws.format("A:A", {"numberFormat": {"type": "DATE_TIME", "pattern": "yyyy-mm-dd hh:mm:ss"}})

                  numeric_formats = [
                      ("F:F", "0.00"), ("N:N", "0.00"), ("V:V", "0.00"),
                      ("G:G", "0"),   ("O:O", "0"),   ("W:W", "0"),
                      ("H:H", "0"),   ("P:P", "0"),   ("X:X", "0"),
                      ("I:I", "0"),   ("Q:Q", "0"),   ("Y:Y", "0"),
                      ("J:J", "0"),   ("R:R", "0"),   ("Z:Z", "0"),
                      ("K:K", "0.0000"), ("S:S", "0.0000"), ("AA:AA", "0.0000"),
                      ("L:L", "0.0000"), ("T:T", "0.0000"), ("AB:AB", "0.0000"),
                  ]

                  for col_range, pattern in numeric_formats:
                      ws.format(col_range, {"numberFormat": {"type": "NUMBER", "pattern": pattern}})

                  ws.spreadsheet.batch_update({
                      "requests": [
                          {
                              "repeatCell": {
                                  "range": {
                                      "sheetId": ws.id,
                                      "startRowIndex": 0,
                                      "endRowIndex": 1,
                                      "startColumnIndex": 0,
                                      "endColumnIndex": len(headers),
                                  },
                                  "cell": {
                                      "userEnteredFormat": {
                                          "textFormat": {"bold": True},
                                          "backgroundColor": {"red": 0.95, "green": 0.95, "blue": 0.95},
                                          "padding": {"top": 2, "bottom": 2, "left": 6, "right": 6},
                                      }
                                  },
                                  "fields": "userEnteredFormat(textFormat,backgroundColor,padding)",
                              }
                          },
                          {
                              "repeatCell": {
                                  "range": {
                                      "sheetId": ws.id,
                                      "startRowIndex": 1,
                                      "endRowIndex": ws.row_count,
                                      "startColumnIndex": 0,
                                      "endColumnIndex": len(headers),
                                  },
                                  "cell": {
                                      "userEnteredFormat": {
                                          "textFormat": {"bold": False},
                                          "padding": {"top": 2, "bottom": 2, "left": 4, "right": 4},
                                      }
                                  },
                                  "fields": "userEnteredFormat(textFormat,padding)",
                              }
                          },
                          {
                              "addConditionalFormatRule": {
                                  "rule": {
                                      "ranges": [{
                                          "sheetId": ws.id,
                                          "startRowIndex": 1,
                                          "endRowIndex": ws.row_count,
                                          "startColumnIndex": 3,
                                          "endColumnIndex": 4,
                                      }],
                                      "booleanRule": {
                                          "condition": {
                                              "type": "TEXT_EQ",
                                              "values": [{"userEnteredValue": "DockAI"}],
                                          },
                                          "format": {
                                              "backgroundColor": {"red": 0.90, "green": 0.97, "blue": 0.91},
                                              "textFormat": {"bold": True},
                                          },
                                      },
                                  },
                              },
                          },
                          {
                              "repeatCell": {
                                  "range": {
                                      "sheetId": ws.id,
                                      "startRowIndex": 1,
                                      "endRowIndex": ws.row_count,
                                      "startColumnIndex": 11,
                                      "endColumnIndex": 12,
                                  },
                                  "cell": {"userEnteredFormat": {"textFormat": {"bold": True}}},
                                  "fields": "userEnteredFormat.textFormat.bold",
                              }
                          },
                          {
                              "repeatCell": {
                                  "range": {
                                      "sheetId": ws.id,
                                      "startRowIndex": 1,
                                      "endRowIndex": ws.row_count,
                                      "startColumnIndex": 19,
                                      "endColumnIndex": 20,
                                  },
                                  "cell": {"userEnteredFormat": {"textFormat": {"bold": True}}},
                                  "fields": "userEnteredFormat.textFormat.bold",
                              }
                          },
                          {
                              "repeatCell": {
                                  "range": {
                                      "sheetId": ws.id,
                                      "startRowIndex": 1,
                                      "endRowIndex": ws.row_count,
                                      "startColumnIndex": 27,
                                      "endColumnIndex": 28,
                                  },
                                  "cell": {"userEnteredFormat": {"textFormat": {"bold": True}}},
                                  "fields": "userEnteredFormat.textFormat.bold",
                              }
                          },
                          {
                              "repeatCell": {
                                  "range": {
                                      "sheetId": ws.id,
                                      "startRowIndex": 1,
                                      "endRowIndex": ws.row_count,
                                      "startColumnIndex": 35,
                                      "endColumnIndex": 36,
                                  },
                                  "cell": {"userEnteredFormat": {"textFormat": {"bold": True}}},
                                  "fields": "userEnteredFormat.textFormat.bold",
                              }
                          },
                          {
                              "autoResizeDimensions": {
                                  "dimensions": {
                                      "sheetId": ws.id,
                                      "dimension": "COLUMNS",
                                      "startIndex": 0,
                                      "endIndex": len(headers),
                                  }
                              }
                          }
                      ]
                  })
              except Exception as exc:
                  print(f"Formatting skipped: {exc}", file=sys.stderr)

          now = datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
          winner = get(data, "notes", "winner")

          base = get(data, "baseline", default={})
          human = get(data, "human", default={})
          dockai = get(data, "dockai", default={})
          codex = get(data, "codex", default={})

          def row_for(obj):
              return [
                  get(obj, "status"),
                  to_mb(get(obj, "size")),
                  get(obj, "time"),
                  get(obj, "omega"),
                  get(obj, "errors"),
                  get(obj, "warnings"),
                  get(obj, "penalty"),
                  get(obj, "cost"),
              ]

          rows = [[
              now,
              repo_name,
              branch_name,
              winner,
              *row_for(base),
              *row_for(human),
              *row_for(dockai),
              *row_for(codex),
          ]]

          headers = [
              "timestamp_utc",
              "repo",
              "branch",
              "winner",
              "baseline_status",
              "baseline_size_mb",
              "baseline_time_s",
              "baseline_omega",
              "baseline_errors",
              "baseline_warnings",
              "baseline_penalty",
              "baseline_cost",
              "human_status",
              "human_size_mb",
              "human_time_s",
              "human_omega",
              "human_errors",
              "human_warnings",
              "human_penalty",
              "human_cost",
              "dockai_status",
              "dockai_size_mb",
              "dockai_time_s",
              "dockai_omega",
              "dockai_errors",
              "dockai_warnings",
              "dockai_penalty",
              "dockai_cost",
              "codex_status",
              "codex_size_mb",
              "codex_time_s",
              "codex_omega",
              "codex_errors",
              "codex_warnings",
              "codex_penalty",
              "codex_cost",
          ]

          scopes = [
              "https://www.googleapis.com/auth/spreadsheets",
              "https://www.googleapis.com/auth/drive.file",
          ]
          creds = Credentials.from_service_account_file("/tmp/sa.json", scopes=scopes)
          client = gspread.authorize(creds)
          sheet = client.open_by_key(sheet_id)
          ws = sheet.worksheet(sheet_tab) if sheet_tab else sheet.sheet1

          try:
              first_row = ws.row_values(1)
          except Exception:
              first_row = []
          if not any(first_row):
              ws.insert_row(headers, 1)

          ws.append_rows(rows, value_input_option="USER_ENTERED")
          format_sheet(ws, headers)
          PY

          python /tmp/update_sheet.py
